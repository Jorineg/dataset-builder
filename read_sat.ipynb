{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorin\\.conda\\envs\\reft\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1000.31ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n",
      "c:\\Users\\jorin\\.conda\\envs\\reft\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jorin\\.cache\\huggingface\\hub\\datasets--jeggers--sat_package_v3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jeggers/sat_package_v3/commit/be8c8c3ed6f0cbea06cb8d12c68c639b43e8c1bd', commit_message='Upload dataset', commit_description='', oid='be8c8c3ed6f0cbea06cb8d12c68c639b43e8c1bd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # The problem format is six pairs of words, where the first pair is the stem\n",
    "# # pair and the next five pairs are the choice pairs. Each pair is on one\n",
    "# # line and the members of the pair are separated by a space. The correct choice\n",
    "# # is indicated by a letter from (a) to (e). Each set of six pairs is preceded\n",
    "# # by a header line and followed by a solution line. Each pair is followed\n",
    "# # by the parts of speech of the two words (n = noun, v = verb, a = adjective,\n",
    "# # r = adverb).\n",
    "# #\n",
    "# # ============================================================================\n",
    "\n",
    "# 190 FROM REAL SATs\n",
    "# lull trust v:n\n",
    "# balk fortitude v:n\n",
    "# betray loyalty v:n\n",
    "# cajole compliance v:n\n",
    "# hinder destination v:n\n",
    "# soothe passion v:n\n",
    "# c\n",
    "\n",
    "# next sample...\n",
    "\n",
    "# read the file SAT-package-V3.txt in the format above to hf datasets\n",
    "\n",
    "with open(\"SAT-package-V3.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# concatenate all lines and split on double newline\n",
    "samples = \"\".join(lines).split(\"\\n\\n\")[:-1]\n",
    "\n",
    "# skip instruction blog\n",
    "samples = samples[1:]\n",
    "\n",
    "def parse_sample(sample):\n",
    "    lines = sample.split(\"\\n\")\n",
    "    source = lines[0]\n",
    "    pairs = lines[1:-1]\n",
    "    # remove word types\n",
    "    pairs = [pair[:-4] for pair in pairs]\n",
    "    # assert all pairs have 2 words\n",
    "    assert all(len(pair.split()) == 2 for pair in pairs), f\"Pairs have more than 2 words: {pairs}\"\n",
    "    # add \"to\" between the pairs\n",
    "    pairs = [\" is to \".join(pair.split()) for pair in pairs]\n",
    "    question = pairs[0]\n",
    "    options = pairs[1:]\n",
    "    solution = lines[-1]\n",
    "    # transform solution to index\n",
    "    solution = ord(solution) - ord(\"a\")\n",
    "    return {\n",
    "        \"source\": source,\n",
    "        \"question\": question,\n",
    "        \"options\": options,\n",
    "        \"solution\": solution\n",
    "    }\n",
    "\n",
    "samples = [parse_sample(sample) for sample in samples]\n",
    "\n",
    "# create a dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    \"question\": [sample[\"question\"] for sample in samples],\n",
    "    \"options\": [sample[\"options\"] for sample in samples],\n",
    "    \"solution\": [sample[\"solution\"] for sample in samples],\n",
    "    \"source\": [sample[\"source\"] for sample in samples],\n",
    "})\n",
    "\n",
    "dataset.push_to_hub(\"jeggers/sat_package_v3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
